{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Discourse Atom Topic Modeling (DATM) Tutorial \n",
    "\n",
    "## Part 1 of 2: Extract Atoms from Word Embedding Trained on your Text Data\n",
    "\n",
    "* This code is written in Python 3.7.2, and uses Gensim version 3.8.3. \n",
    "* This code is provides an the outline of how we identified topics in a word embedding trained on our cleaned data, and then explored the resultings topics. Note that we cannot redistribute the data used in our paper \"Integrating Topic Modeling and Word Embedding\" in any form, and researchers must apply directly to the Centers for Disease Control and Prevention for access. Details on data access are provided in the paper. We add comments with tips for adapting this code to your data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "import pandas as pd\n",
    "import math\n",
    "from gensim.models import coherencemodel\n",
    "import pickle\n",
    "from scipy.linalg import norm\n",
    "from sklearn.preprocessing import normalize\n",
    "from scipy.stats import entropy\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import os\n",
    "from itertools import combinations\n",
    "import numpy as np\n",
    "from gensim import corpora, models, similarities #calc all similarities at once, from http://radimrehurek.com/gensim/tut3.html\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from gensim.models import Word2Vec, KeyedVectors\n",
    "from random import seed, sample\n",
    "import seaborn as sns\n",
    "from ksvd import ApproximateKSVD \n",
    "\n",
    "\n",
    "from quality import reconst_qual, topic_diversity, coherence_centroid, coherence_pairwise #written for this jupyter notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Input: Word2Vec model Trained on your Text Data\n",
    "\n",
    "* Below, we use a public, and free word2vec model pretrained on Google News to illustrate how to identify and explore atom vectors in a trained embedding space. [To download the model, click here](https://drive.google.com/file/d/0B7XkCwpI5KDYNlNUTTlSS21pQmM/edit?usp=sharing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "currentmodel = KeyedVectors.load_word2vec_format('C:/Users/arsen/Dropbox/GSRM/LexisNexis Data/GoogleW2V/GoogleNews-vectors-negative300.bin.gz', limit=40000, binary=True) \n",
    "#change to the working directory where you downloaded your model #in this tutorial, we're limiting the w2v model to the top 40k words for efficienc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract Atoms with K-SVD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_aksvd(w2vmodel, n_comp, n_nonzeros,  savelocation, save=False ):\n",
    "    aksvd_t = ApproximateKSVD(n_components=n_comp, transform_n_nonzero_coefs=n_nonzeros) #n_components is number of discourse atoms, if vocab size is smallish, keep this fewer. transform_n is the number of atoms (components) that a word can be a linear combo of\n",
    "    dictionary_t = aksvd_t.fit(w2vmodel.wv.vectors).components_ # Dictionary is the matrix of discourse atoms.\n",
    "    gamma_t = aksvd_t.transform(w2vmodel.wv.vectors) #get the gammas, which are the \"weights\" of each word on a discourse atoms\n",
    "    #len(dictionary[0]) #check that a discourse-atom vector is still same dimensions as word-vectors, note that norm of the dictionary vecs (atoms) are each 1! \n",
    "    if save==True:\n",
    "        outfile = open(str(savelocation)  + str(n_comp) + 'comp' + str(n_nonzeros) + 'nonzeros_aksvd','wb')\n",
    "        pickle.dump(aksvd_t,outfile)\n",
    "        outfile.close()\n",
    "        \n",
    "        outfile = open(str(savelocation)  +str(n_comp) + 'comp' + str(n_nonzeros) + 'nonzeros_dictionary' ,'wb')\n",
    "        pickle.dump(dictionary_t,outfile)\n",
    "        outfile.close()\n",
    "        \n",
    "        outfile = open(str(savelocation)  + str(n_comp) + 'comp' + str(n_nonzeros) + 'nonzeros_gamma','wb')\n",
    "        pickle.dump(gamma_t,outfile)\n",
    "        outfile.close()\n",
    "    return(dictionary_t, gamma_t)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sample usage:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mydictionary, mygamma = do_aksvd(currentmodel, 200, 5,  os.getcwd(),  save=False) #200 topics, each word can be a linear combo of 5 topics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate Internal Model Quality\n",
    "\n",
    "* Using coherence, topic diversity, sse, rmse, or $r^2$\n",
    "* These functions are imported from quality .py file, above, see code in this .py file or [paper](https://arxiv.org/abs/2106.14365) for details on these functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coherence (pairwise): 0.4617589\n",
      "Topic Diversity: 0.9562\n",
      "SSE, RMSE, R2: (282498.41000679834, 0.15343250687919166, 0.4359828777277901)\n"
     ]
    }
   ],
   "source": [
    "print('Coherence (pairwise):', coherence_pairwise(currentmodel, mydictionary, top_n=25))\n",
    "\n",
    "print('Topic Diversity:', topic_diversity(currentmodel, mydictionary, top_n=25))\n",
    "\n",
    "print('SSE, RMSE, R2:', reconst_qual(currentmodel, mydictionary, mygamma))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Select the number of hyperparameters (e.g., number of atoms and/or number of nonzeros) by training models on a range of these hyperparameters and using quality metrics to select the best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ntopics= []\n",
    "nonzeros = []\n",
    "cohere_pairwise= []\n",
    "div=[]\n",
    "sse= []\n",
    "rmse =[]\n",
    "r2=[]\n",
    "\n",
    "\n",
    "for i in [25, 50, 75, 100, 200]: \n",
    "    for j in [2,5]:\n",
    "        dictionary, gamma = do_aksvd(currentmodel, i, j, os.getcwd(),  save=True) #varying hyperparameters\n",
    "        cohere_pairwise.append(coherence_pairwise(currentmodel, dictionary, top_n=25))\n",
    "        div.append(topic_diversity(currentmodel, dictionary, top_n=25))\n",
    "        rec= reconst_qual(currentmodel, dictionary, gamma)\n",
    "        sse.append(rec[0])\n",
    "        rmse.append(rec[1])\n",
    "        r2.append(rec[2])\n",
    "        ntopics.append(i)\n",
    "        nonzeros.append(j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "quality_results = pd.DataFrame(data={'Components_Topics': ntopics,'Nonzeros': nonzeros,\n",
    "                'CohereCossim_top25_mean': cohere_pairwise, 'Diversity_top25': div, \n",
    "                  'SSE': sse,'RMSE': rmse, 'R2': r2})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set(style=\"ticks\")\n",
    "plt=sns.lineplot(x=\"Components_Topics\", y=\"CohereCossim_top25_mean\", hue=\"Nonzeros\", data=quality_results, legend='full', sort=True)\n",
    "plt.legend(bbox_to_anchor=(1.3, .5),loc='center right')#, borderaxespad=0.) \n",
    "#plt.set(ylim=(.67, .85))\n",
    "#plt.set(xlim=(0,550))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploring the Model and Resulting Topics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exploring the 25 most similar words to each atom atom and their respective cosine similarities (note that here is where you get a \"topic\": the distribution of words that characterize an atom vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0, len(mydictionary)): \n",
    "    print(\"Discourse_Atom \" + str(i))\n",
    "    print([j for j in currentmodel.wv.similar_by_vector(mydictionary[i],topn=25)]) #what are the 25 most similar words to the Nth dicourse atom?\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extract a Gender Dimension and Compute the Gender Loading of the Topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "manvec = np.mean([currentmodel.wv['male'],  currentmodel.wv['man'], currentmodel.wv['he'], currentmodel.wv['his'], currentmodel.wv['him'], currentmodel.wv['himself']], axis=0)\n",
    "womanvec= np.mean([currentmodel.wv['female'],  currentmodel.wv['woman'], currentmodel.wv['she'], currentmodel.wv['hers'], currentmodel.wv['her'], currentmodel.wv['herself']], axis=0)\n",
    "\n",
    "gendervec= normalize(womanvec.reshape(1, -1))-normalize(manvec.reshape(1, -1))\n",
    "\n",
    "cossim_gender=[]\n",
    "for i in range(0, len(mydictionary)):\n",
    "    print(\"Discourse_Atom \" + str(i))\n",
    "    print([i for i in currentmodel.wv.similar_by_vector(mydictionary[i],topn=25)]) #what are the most similar words to the ith discourse atom?\n",
    "    print(cosine_similarity(gendervec.reshape(1,-1), mydictionary[i].reshape(1,-1))[0])\n",
    "    cossim_gender.append(cosine_similarity(gendervec.reshape(1,-1), mydictionary[i].reshape(1,-1))[0])\n",
    "    #print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#look at the topics with the largest loading on this dimension (the scalar indicates strength of the loading, the sign indicates direction - whether on the feminine or masculine side)\n",
    "zippes= zip( cossim_gender, [i for i in range(0, len(mydictionary))]) #get most fem/masc\n",
    "sorted(zippes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#write results to CSV\n",
    "\n",
    "genderedlevels= pd.DataFrame(np.concatenate( cossim_gender, axis=0 ), columns= ['gendered_connotation'])\n",
    "genderedlevels.to_csv('gendered_connotations_of_topics.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extract an Indoor/Outdoor Dimension and Compute the Loading of the Topics on this Dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indoorvec = np.mean([currentmodel.wv['indoor'],  currentmodel.wv['indoors'] , currentmodel.wv['inside']], axis=0)\n",
    "outdoorvec= np.mean([currentmodel.wv['outdoor'], currentmodel.wv['outdoors'], currentmodel.wv['outside']], axis=0)\n",
    "\n",
    "indooroutdoorvec= normalize(indoorvec.reshape(1, -1))-normalize(outdoorvec.reshape(1, -1))\n",
    "\n",
    "cossim_indout=[]\n",
    "for i in range(0, len(mydictionary)):\n",
    "    print(\"Discourse_Atom \" + str(i))\n",
    "    print([i[0] for i in currentmodel.wv.similar_by_vector(mydictionary[i],topn=15)]) #what are the most similar words to the Nth dicourse atom?\n",
    "    print(cosine_similarity(indooroutdoorvec.reshape(1,-1), mydictionary[i].reshape(1,-1))[0])\n",
    "    cossim_indout.append(cosine_similarity(indooroutdoorvec.reshape(1,-1), mydictionary[i].reshape(1,-1))[0])\n",
    "    #print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#look at the topics with the largest loading on this dimension (the scalar indicates strength of the loading, the sign indicates direction - whether indoor or outdoor)\n",
    "zippes= zip( cossim_indout, [i for i in range(0, len(mydictionary))]) \n",
    "sorted(zippes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#write results to CSV\n",
    "\n",
    "indoutlevels= pd.DataFrame(np.concatenate( cossim_indout, axis=0 ), columns= ['indooroutdoor_connotation'])\n",
    "indoutlevels.to_csv('indout_connotations_of_topics.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
